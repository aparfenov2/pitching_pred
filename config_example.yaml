optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 0.01

lr_scheduler:
  # class_path: torch.optim.lr_scheduler.ExponentialLR
  # init_args:
  #   gamma: 0.99
  #   verbose: true

  class_path: torch.optim.lr_scheduler.StepLR
  init_args:
    step_size: 10
    gamma: 0.8
    verbose: true

trainer:
  max_epochs: 100
  callbacks: []

    # - class_path: pytorch_lightning.callbacks.EarlyStopping
    #   init_args:
    #     monitor: val_loss
    #     min_delta: 1e-4
    #     patience: 10
    #     verbose: false
    #     mode: min

model:
  hidden_sz: 10
  input_sz: 2

data:
  batch_size: 32
  base_freq: 50
  freq: 12.5
  cols: [KK, KK_v]
  fn_train: data/NPN_1155_part2.dat
  fn_test:  data/NPN_1155_part1.dat
