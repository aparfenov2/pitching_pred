optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 0.01

lr_scheduler:
  # class_path: torch.optim.lr_scheduler.ExponentialLR
  # init_args:
  #   gamma: 0.99
  #   verbose: true

  class_path: torch.optim.lr_scheduler.StepLR
  init_args:
    step_size: 10
    gamma: 0.8
    verbose: true

trainer:
  callbacks: []

    # - class_path: pytorch_lightning.callbacks.EarlyStopping
    #   init_args:
    #     monitor: val_loss
    #     min_delta: 1e-4
    #     patience: 10
    #     verbose: false
    #     mode: min
